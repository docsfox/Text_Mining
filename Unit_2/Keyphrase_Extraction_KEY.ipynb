{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **In-Class Assignment: Keyphrase Extraction**\n",
        "\n",
        "## *IS 5150*\n",
        "## Name: KEY\n",
        "\n",
        "In this first in-class assignment for Topic 6, we will learn the most basic form of document summarization: key phrase extraction. We will go through an example of pulling key phrases via collocation of various sized n-grams, and then utilize some annotation methods to perform key-phrase extracted via weighted tag-based phrase extraction.\n",
        "\n",
        "Let's start with finding collocations in *Alice in Wonderland*, which we can access through the `gutenberg` corpus. We will use several functions from `nltk` to extract common bi-grams (two word combos) and tri-grams (three word combos).\n",
        "\n",
        "First, we will import our dependencies, we can access the text_normalizer function from my github repo [here](https://github.com/docsfox/Text_Mining/)."
      ],
      "metadata": {
        "id": "EEhPDTFtJMDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) Setting up the Corpus**"
      ],
      "metadata": {
        "id": "ilVERjp0V5VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text Normalizer Function"
      ],
      "metadata": {
        "id": "X0o3crgrWCOf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNjlB8MdJCQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e3dbae4-7268-475c-b67e-7b05bd793ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.72)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
          ]
        }
      ],
      "source": [
        "# text normalizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')                                                                                            # dependencies\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]                                                                         # html parsing\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def tokenize_text(text):                                                                                                      # text tokenization\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] \n",
        "    return word_tokens\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')                            # accent removal\n",
        "    return text\n",
        "\n",
        "def expand_contractions(text):                                                                                                # expand contractions\n",
        "    expanded_words = []\n",
        "    for word in text.split():\n",
        "        expanded_words.append(contractions.fix(word))\n",
        "        expanded_text = ' '.join(expanded_words)\n",
        "    return expanded_text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):                                                                    # special character removal\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def simple_stemmer(text):                                                                                                   # stemmer\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    text = nlp(text)                                                                                                        # lemmatizer\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):                                                   # stopword removal\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,                                                # define normalize corpus function\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=True, remove_digits=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # strip HTML\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "        # remove accented characters\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "        # expand contractions    \n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # lemmatize text\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "        \n",
        "    return normalized_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Other dependencies"
      ],
      "metadata": {
        "id": "hKuH9MzwWAhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download() 'punkt', 'gutenberg', 'averaged_perceptron_tagger\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "from nltk.collocations import TrigramAssocMeasures\n",
        "\n",
        "from operator import itemgetter\n",
        "import itertools\n",
        "\n",
        "from gensim import corpora, models\n",
        "from gensim.summarization import keywords"
      ],
      "metadata": {
        "id": "w4e5VsN9WK5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's bring in our Alice text from the `gutenberg` corpus and then go ahead and apply our normalize_corpus function to clean the text."
      ],
      "metadata": {
        "id": "Pr5OMLUZWZRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
        "alice = [' '.join(ts) for ts in alice]\n",
        "norm_alice = list(filter(None, normalize_corpus(alice, text_lemmatization=False)))"
      ],
      "metadata": {
        "id": "RQnFAWpaWqG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_alice[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r30AlW2vXRNF",
        "outputId": "33803989-3df9-47f7-b54b-d629ed42da54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[ alice adventures wonderland lewis carroll ]',\n",
              " 'chapter',\n",
              " 'rabbit hole',\n",
              " 'alice beginning get tired sitting sister bank nothing twice peeped book sister reading pictures conversations use book thought alice without pictures conversation',\n",
              " 'considering mind well could hot day made feel sleepy stupid whether pleasure making daisy chain would worth trouble getting picking daisies suddenly white rabbit pink eyes ran close',\n",
              " 'nothing remarkable alice think much way hear rabbit say oh dear',\n",
              " 'oh dear',\n",
              " 'shall late',\n",
              " 'thought afterwards occurred ought wondered time seemed quite natural rabbit actually took watch waistcoat pocket looked hurried alice started feet flashed across mind never seen rabbit either waistcoat pocket watch take burning curiosity ran across field fortunately time see pop large rabbit hole hedge',\n",
              " 'another moment went alice never considering world get']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why would we set lemmatization to false, if the end goal is to find frequent collocations?**"
      ],
      "metadata": {
        "id": "zUvLrHoMXbc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may want to retain information about tense or other inflection information so we can best represent the meaning of key phrases (i.e., \"the cat pajamas\" vs. \"the cats pajamas\") shouldn't be treated as the same phrase."
      ],
      "metadata": {
        "id": "oPT_YSv3Xl3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Defining an N-Grams function**\n",
        "\n",
        "Next we will define a function for computing n-grams, so that we can group collocations based on different sizes of n; n = 1 is a unigram, n = 2 is a bi-gram, etc."
      ],
      "metadata": {
        "id": "I_k1eXaRaBKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ngrams(sequence, n):\n",
        "    return list(\n",
        "            zip(*(sequence[index:] \n",
        "                     for index in range(n)))\n",
        "    )"
      ],
      "metadata": {
        "id": "V8VKVFyxaSyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see what it does\n",
        "\n",
        "print(\"bigrams:\", compute_ngrams([\"the\", \"broken\", \"door\", \"hinge\"], 2))            # bigrams\n",
        "print(\"trigrams:\", compute_ngrams([\"the\", \"broken\", \"door\", \"hinge\"], 3))           # trigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsVlyBfjaWrm",
        "outputId": "6a8f2c3f-01d6-4c89-fd63-e92cd21413e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams: [('the', 'broken'), ('broken', 'door'), ('door', 'hinge')]\n",
            "trigrams: [('the', 'broken', 'door'), ('broken', 'door', 'hinge')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) Generate Top N-grams**\n",
        "\n",
        "Before we can generate the most frequent n-grams, we need to \"flatten\" our corpus inot one continuous string, so that we can find the most frequent n-grams over the whole text.\n",
        "\n",
        "To get the most frequent n-grams, we will combine our flatten text function, our compute_ngrams function, and some existing functions from `FreqDist` in `nltk` to examine the frequency of different n-grams and then return the most frequent."
      ],
      "metadata": {
        "id": "gAuagyW9a7Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_corpus(corpus):\n",
        "    return ' '.join([document.strip() \n",
        "                     for document in corpus])\n",
        "    \n",
        "\n",
        "def get_top_ngrams(corpus, ngram_val=1, limit=5):                                                     # default parameters are unigram, top 5 most frequent\n",
        "\n",
        "    corpus = flatten_corpus(corpus)                                                                   # convert corpus to one long string\n",
        "    tokens = nltk.word_tokenize(corpus)                                                               # tokenize the flattened corpus to word tokens\n",
        "\n",
        "    ngrams = compute_ngrams(tokens, ngram_val)                                                        # compute n_grams on tokens, based on ngram_val value\n",
        "    ngrams_freq_dist = nltk.FreqDist(ngrams)                                                          # find the frequency distribution of the ngrams\n",
        "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(),                                               # sort the nrgams from most to least frequent\n",
        "                              key=itemgetter(1), reverse=True)\n",
        "    sorted_ngrams = sorted_ngrams_fd[0:limit]                                                         # select first in list, up to the limit\n",
        "    sorted_ngrams = [(' '.join(text), freq)                                                           # join together the n-grams into a list\n",
        "                     for text, freq in sorted_ngrams]\n",
        "\n",
        "    return sorted_ngrams"
      ],
      "metadata": {
        "id": "i2PY6HCEbSw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's find the top 10 bigrams. What parameters do I need to change?**"
      ],
      "metadata": {
        "id": "3Vnny9BucjUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_top_ngrams(corpus=norm_alice, ngram_val=2, limit=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNGLObt4cbNa",
        "outputId": "950a36ba-3b84-43d2-a086-c1fa9b708a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('said alice', 123),\n",
              " ('mock turtle', 56),\n",
              " ('march hare', 31),\n",
              " ('said king', 29),\n",
              " ('thought alice', 26),\n",
              " ('white rabbit', 22),\n",
              " ('said hatter', 22),\n",
              " ('said mock', 20),\n",
              " ('said caterpillar', 18),\n",
              " ('said gryphon', 18)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next, let's find the top 10 trigrams. What information do we ascertain about the text from these n-grams?**"
      ],
      "metadata": {
        "id": "bl_ieNU0c5gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_top_ngrams(corpus=norm_alice, ngram_val=3, limit=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8fUAGzgdAoh",
        "outputId": "9f14dede-2b26-48f6-c47c-733cb094bcc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('said mock turtle', 20),\n",
              " ('said march hare', 10),\n",
              " ('poor little thing', 6),\n",
              " ('little golden key', 5),\n",
              " ('certainly said alice', 5),\n",
              " ('white kid gloves', 5),\n",
              " ('march hare said', 5),\n",
              " ('mock turtle said', 5),\n",
              " ('know said alice', 4),\n",
              " ('might well say', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appears to be primarily dialogue, which might indicate that there is a lot of dialogue in the book. In addition, it gives us an idea of the main characters in the book. It doesn't tell us much about the plot though."
      ],
      "metadata": {
        "id": "6PMgMsGSdHTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can also utilize `nltk` collaction finder functions if we want to go beyond finding just the raw frequencies, like so:**\n",
        "\n",
        "You can find the other metrics of collocation in the documentation [here](https://tedboy.github.io/nlps/generated/generated/nltk.BigramAssocMeasures.html)"
      ],
      "metadata": {
        "id": "ta2IzJx0eU1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finder = BigramCollocationFinder.from_documents([item.split() \n",
        "                                                for item \n",
        "                                                in norm_alice])\n",
        "finder\n",
        "\n",
        "print(\"Raw Frequencies:\", finder.nbest(BigramAssocMeasures.raw_freq, 10))\n",
        "print(\"PMI:\", finder.nbest(BigramAssocMeasures.pmi, 10))\n",
        "print(\"MI_like:\", finder.nbest(BigramAssocMeasures.mi_like, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7guc223OeddF",
        "outputId": "1338db33-2b0e-4def-a19d-00234b1c314f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Frequencies: [('said', 'alice'), ('mock', 'turtle'), ('march', 'hare'), ('said', 'king'), ('thought', 'alice'), ('said', 'hatter'), ('white', 'rabbit'), ('said', 'mock'), ('said', 'caterpillar'), ('said', 'gryphon')]\n",
            "PMI: [('abide', 'figures'), ('acceptance', 'elegant'), ('accounting', 'tastes'), ('accustomed', 'usurpation'), ('act', 'crawling'), ('adjourn', 'immediate'), ('adoption', 'energetic'), ('affair', 'trusts'), ('agony', 'terror'), ('alarmed', 'proposal')]\n",
            "MI_like: [('mock', 'turtle'), ('march', 'hare'), ('said', 'alice'), ('soo', 'oop'), ('white', 'rabbit'), ('join', 'dance'), ('beg', 'pardon'), ('beau', 'ootiful'), ('mary', 'ann'), ('yer', 'honour')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finder = TrigramCollocationFinder.from_documents([item.split() \n",
        "                                                for item \n",
        "                                                in norm_alice])\n",
        "\n",
        "print(\"Raw Frequencies:\", finder.nbest(TrigramAssocMeasures.raw_freq, 10))\n",
        "print(\"PMI:\", finder.nbest(TrigramAssocMeasures.pmi, 10))\n",
        "print(\"MI_like:\", finder.nbest(TrigramAssocMeasures.mi_like, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4jHNZs3gUN_",
        "outputId": "3964a2e9-1e8e-44da-80e3-6d71bcf1c5cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Frequencies: [('said', 'mock', 'turtle'), ('said', 'march', 'hare'), ('poor', 'little', 'thing'), ('little', 'golden', 'key'), ('march', 'hare', 'said'), ('mock', 'turtle', 'said'), ('white', 'kid', 'gloves'), ('beau', 'ootiful', 'soo'), ('certainly', 'said', 'alice'), ('might', 'well', 'say')]\n",
            "PMI: [('accustomed', 'usurpation', 'conquest'), ('adjourn', 'immediate', 'adoption'), ('adoption', 'energetic', 'remedies'), ('ancient', 'modern', 'seaography'), ('apple', 'roast', 'turkey'), ('arithmetic', 'ambition', 'distraction'), ('brother', 'latin', 'grammar'), ('canvas', 'bag', 'tied'), ('cherry', 'tart', 'custard'), ('circle', 'exact', 'shape')]\n",
            "MI_like: [('accustomed', 'usurpation', 'conquest'), ('adjourn', 'immediate', 'adoption'), ('adoption', 'energetic', 'remedies'), ('ancient', 'modern', 'seaography'), ('apple', 'roast', 'turkey'), ('arithmetic', 'ambition', 'distraction'), ('brother', 'latin', 'grammar'), ('canvas', 'bag', 'tied'), ('cherry', 'tart', 'custard'), ('circle', 'exact', 'shape')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What, if any additional information do we glean about the topic/plot of the text that would be helpful for text summarization?**"
      ],
      "metadata": {
        "id": "a7TLszGxgFjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fGkA6NFIgrZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Weighted Tag-Based Phrase Extration**\n",
        "\n",
        "We will use a different text source for this example; instead using some wikipedia text about elephants (why not)!\n",
        "\n"
      ],
      "metadata": {
        "id": "sSvehL1ggr7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) Setting up the corpus**"
      ],
      "metadata": {
        "id": "3nk5vdP_tdja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('/content/elephants.txt', 'r+').readlines()\n",
        "sentences = nltk.sent_tokenize(data[0])\n",
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT-6GrQctPez",
        "outputId": "bc3a0d99-68db-40f2-9ae5-e812d6bf86e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNCve_tKtRbm",
        "outputId": "73f3ccf6-3ec0-4fd3-a4b8-e60d88ed1b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Elephants are large mammals of the family Elephantidae and the order Proboscidea.',\n",
              " 'Three species are currently recognised: the African bush elephant (Loxodonta africana), the African forest elephant (L. cyclotis), and the Asian elephant (Elephas maximus).',\n",
              " 'Elephants are scattered throughout sub-Saharan Africa, South Asia, and Southeast Asia.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thinking about the fact that we want to extract noun phrases from our corpus AND retain meaning of the key phrases we extract, what are some of the normalization steps we might want to skip and why?**"
      ],
      "metadata": {
        "id": "ucRLzZ0Fthvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Capitals can help us identify proper nouns better, stopwords similarly may be important for parsing (ID noun phrase by determiner noun), lemmatization can reduce word meaning in key phrase interpretation."
      ],
      "metadata": {
        "id": "tAq4Eeqxtv__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_sentences = normalize_corpus(sentences, text_lower_case=False, \n",
        "                                  text_lemmatization=False, stopword_removal=False)\n",
        "norm_sentences[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4HsXhA2ttyp",
        "outputId": "12f98d81-a80b-4226-80b8-307e3ee5e257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Elephants are large mammals of the family Elephantidae and the order Proboscidea ',\n",
              " 'Three species are currently recognised the African bush elephant Loxodonta africana the African forest elephant L cyclotis and the Asian elephant Elephas maximus ',\n",
              " 'Elephants are scattered throughout subSaharan Africa South Asia and Southeast Asia ']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Extract all noun phrase chunk using shallow parsing**\n",
        "\n",
        "In our first step of weighted tag-based extraction, we first want to select all of the noun phrases. Remember that shallow parsing relies on basic part-of-speech tags that we can produce using our built-in `nltk.pos_tag`.\n",
        "\n",
        "We will use some regex to identify noun phrases based on the combination of the typical POS that constitute a noun phrase: determiners, adjectives, and nouns.\n",
        "\n",
        "> **What are some noun phrases we could miss out on based on our basic regex?**"
      ],
      "metadata": {
        "id": "QeOQDvfCu5hf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More elaborate noun phrases that have additional modifiers or other tags preceding the noun; also missing post modification of nouns (\"the girl with red hair\")."
      ],
      "metadata": {
        "id": "1nDJjIYQwr6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks(sentences, grammar = r'NP: {<DT>? <JJ>* <NN.*>+}', stopword_list=stopword_list):\n",
        "    all_chunks = []\n",
        "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
        "    for sentence in sentences:      \n",
        "        tagged_sents = [nltk.pos_tag(nltk.word_tokenize(sentence))]      \n",
        "        chunks = [chunker.parse(tagged_sent) \n",
        "                      for tagged_sent in tagged_sents]\n",
        "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
        "                         for chunk in chunks]                                                       # creates a triple of words, tags, and chunk tags\n",
        "        flattened_chunks = list(\n",
        "                            itertools.chain.from_iterable(\n",
        "                                wtc_sent for wtc_sent in wtc_sents)\n",
        "                           )\n",
        "        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) \n",
        "                                   for status, chunk \n",
        "                                       in itertools.groupby(flattened_chunks, \n",
        "                                                lambda word_pos_chunk: word_pos_chunk[2] != 'O')]   # remove all tags with chunk tag  = 0\n",
        "        valid_chunks = [' '.join(word.lower() \n",
        "                                for word, tag, chunk in wtc_group \n",
        "                                    if word.lower() not in stopword_list)                           # generate phrases from each chunk group\n",
        "                                        for status, wtc_group in valid_chunks_tagged\n",
        "                                            if status]                                   \n",
        "        all_chunks.append(valid_chunks)\n",
        "    return all_chunks"
      ],
      "metadata": {
        "id": "wjH6q9f3vaTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = get_chunks(norm_sentences)\n",
        "chunks"
      ],
      "metadata": {
        "id": "3sD6aDe1xR3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) Compute TF-IDF Weights for each chunk**"
      ],
      "metadata": {
        "id": "3PYL7BA7xyi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tfidf_weighted_keyphrases(sentences, \n",
        "                                  grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',                                         # set regex grammar for shallow parsing\n",
        "                                  top_n=10):\n",
        "    \n",
        "    valid_chunks = get_chunks(sentences, grammar=grammar)                                                       # get valid chunks using get_chunks function\n",
        "                                     \n",
        "    dictionary = corpora.Dictionary(valid_chunks)                                                               # create dictionary for valid_chunks\n",
        "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]                                              # assign valid_chunks as chunks to corpus\n",
        "    \n",
        "    tfidf = models.TfidfModel(corpus)                                                                           # apply tfidf to chunks in corpus\n",
        "    corpus_tfidf = tfidf[corpus]                                                                                \n",
        "    \n",
        "    weighted_phrases = {dictionary.get(idx): value                                                              # create dictionary of chunks and their tf-idf weights\n",
        "                           for doc in corpus_tfidf \n",
        "                               for idx, value in doc}\n",
        "                            \n",
        "    weighted_phrases = sorted(weighted_phrases.items(), \n",
        "                              key=itemgetter(1), reverse=True)                                                 # sort the weighted phrases in descending order\n",
        "    weighted_phrases = [(term, round(wt, 3)) for term, wt in weighted_phrases]                                 # round weights to 3 decimal places\n",
        "    \n",
        "    return weighted_phrases[:top_n]                                                                            # return top_n weighted phrases"
      ],
      "metadata": {
        "id": "ubb8MJ6Nx4a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4) Return top weighted phrases**"
      ],
      "metadata": {
        "id": "JFrG3sesy3yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_tfidf_weighted_keyphrases(sentences = norm_sentences, top_n=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTuDcPYNyUpo",
        "outputId": "2d65ee98-88f3-4a82-fd19-5d4eb4ba3c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('water', 1.0),\n",
              " ('asia', 0.807),\n",
              " ('wild', 0.764),\n",
              " ('great weight', 0.707),\n",
              " ('pillarlike legs', 0.707),\n",
              " ('southeast asia', 0.693),\n",
              " ('subsaharan africa south asia', 0.693),\n",
              " ('body temperature', 0.693),\n",
              " ('flaps', 0.693),\n",
              " ('fissionfusion society', 0.693),\n",
              " ('multiple family groups', 0.693),\n",
              " ('art folklore religion literature', 0.693),\n",
              " ('popular culture', 0.693),\n",
              " ('ears', 0.681),\n",
              " ('males', 0.653),\n",
              " ('males bulls', 0.653),\n",
              " ('family elephantidae', 0.607),\n",
              " ('large mammals', 0.607),\n",
              " ('years', 0.607),\n",
              " ('environments', 0.577),\n",
              " ('impact', 0.577),\n",
              " ('keystone species', 0.577),\n",
              " ('cetaceans', 0.577),\n",
              " ('elephant intelligence', 0.577),\n",
              " ('primates', 0.577),\n",
              " ('dead individuals', 0.577),\n",
              " ('kind', 0.577),\n",
              " ('selfawareness', 0.577),\n",
              " ('different habitats', 0.57),\n",
              " ('marshes', 0.57)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What sorts of insights can we derive about the content of this text based on these key phrases? How would you compare these to the n-gram collocation key phrases?**"
      ],
      "metadata": {
        "id": "Oa8tRU0xy_Ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TJeQPfC1zIyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gensim Keyword Function**\n",
        "\n",
        "We will talk more about this function in the automated document summarization notebook, but here is one more (fast) way to extract keywords from the text that is built-in to `gensim`."
      ],
      "metadata": {
        "id": "xWV2K0RQzJO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key_words = keywords(data[0], ratio=1.0, scores=True, lemmatize=True)\n",
        "[(item, round(score, 3)) for item, score in key_words][:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQAUU_ZuzifB",
        "outputId": "922b986c-7425-44fc-8c19-2cb8efdcf34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('african bush elephant', 0.261),\n",
              " ('including', 0.141),\n",
              " ('family', 0.137),\n",
              " ('cow', 0.124),\n",
              " ('forests', 0.108),\n",
              " ('female', 0.103),\n",
              " ('asia', 0.102),\n",
              " ('tigers', 0.098),\n",
              " ('ivory', 0.098),\n",
              " ('sight', 0.098),\n",
              " ('objects', 0.098),\n",
              " ('males', 0.088),\n",
              " ('known', 0.087),\n",
              " ('religion', 0.087),\n",
              " ('folklore', 0.087),\n",
              " ('larger ears', 0.085),\n",
              " ('water', 0.075),\n",
              " ('highly recognisable', 0.075),\n",
              " ('breathing lifting', 0.074),\n",
              " ('flaps', 0.073),\n",
              " ('africa', 0.072),\n",
              " ('gomphotheres', 0.072),\n",
              " ('animals tend', 0.071),\n",
              " ('success', 0.071),\n",
              " ('south', 0.07),\n",
              " ('habitat destruction', 0.068),\n",
              " ('elephantidae', 0.068),\n",
              " ('increased testosterone', 0.067),\n",
              " ('iucn', 0.067),\n",
              " ('biggest threats', 0.067)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This method pulled some of the same and also some different keywords/phrases, and also applied a different scoring/weighting mechanism. What are some of the differences in the keywords/phrases that were identified between the two?**"
      ],
      "metadata": {
        "id": "pp3c97a2zxQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nrcHemZp0N8X"
      }
    }
  ]
}