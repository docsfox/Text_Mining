{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0c8ca122",
      "metadata": {
        "id": "0c8ca122"
      },
      "source": [
        "# **In-Class Demonstration: NLP Pipeline for EBooks, Static HTML, Wikipedia**\n",
        "\n",
        "## *IS 5150*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this in-class assignment we will run several sources of text through the first few steps of the NLP pipeline, including:\n",
        "\n",
        "\n",
        "\n",
        "1.   Reading in raw text\n",
        "2.   Decoding text to readable format/removal of HTML tags\n",
        "3.   Tokenizing\n",
        "4.   Trimming unwanted tokens\n",
        "\n",
        "We will start with one of the cleaner sources of text, Ebooks, and then delve into webpages, which present the additional issue of extracting meaningful text from a sea of HTML.\n",
        "\n"
      ],
      "metadata": {
        "id": "lB9Nq_NbbhMx"
      },
      "id": "lB9Nq_NbbhMx"
    },
    {
      "cell_type": "markdown",
      "id": "19cd4833",
      "metadata": {
        "id": "19cd4833"
      },
      "source": [
        "### **Let's start with an E-Book and run through these steps...**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f813837",
      "metadata": {
        "id": "2f813837"
      },
      "source": [
        "We first need to import our required libraries and packages. We'll need `nltk`, `re`, and `pprint`. We will also want to import `word_tokenize` from `nltk`, and then `request` from `urllib`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98f597f2",
      "metadata": {
        "id": "98f597f2"
      },
      "outputs": [],
      "source": [
        "import nltk, re, pprint\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from urllib import request"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b7789f",
      "metadata": {
        "id": "87b7789f"
      },
      "source": [
        "#### **1) Reading in raw ebook text**\n",
        "#### **2) Decode text to readable format**\n",
        "\n",
        "The first step in the pipeline is to actually read in the text we want to process. This can be done using the `request.urlopen()` function from `urllib`. We're also going to convert this raw text to `utf-8-sig` so that all the characters are in a computer readable format, using the `decode` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11d91bed",
      "metadata": {
        "id": "11d91bed",
        "outputId": "0b35a7e0-67c2-4a26-d2d6-6efb210630fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data type: <class 'str'> Number of characters raw: 1176811\n"
          ]
        }
      ],
      "source": [
        "url = \"https://www.gutenberg.org/files/2554/2554-0.txt\"                                                           # provide url of ebook\n",
        "response = request.urlopen(url)                                                                                   # open url\n",
        "\n",
        "raw = response.read().decode('utf-8-sig')                                                                         # decode raw text to utf-8 encoding\n",
        "\n",
        "print(\"data type:\", type(raw), \"Number of characters raw:\", len(raw))                                             # print data type and number of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "486a3206",
      "metadata": {
        "id": "486a3206",
        "outputId": "ffe75870-cea7-483c-915e-77ddf5ceb767"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw[:75]                                                                                                          # extract the title and author of the text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0355d563",
      "metadata": {
        "id": "0355d563"
      },
      "source": [
        "#### **3) Tokenization**\n",
        "\n",
        "Now that we've got our text decode and in a readable format, let's tokenize our text so that we can break down continuous strings of characters into words. Luckily, `nltk` has a built-in tokenizer called `word_tokenize()`. Let's apply it to our raw text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4c2950f",
      "metadata": {
        "id": "d4c2950f",
        "outputId": "ef806556-69f4-4523-a470-eb2ee9c08f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data type: <class 'list'> Number of tokens: 257058\n"
          ]
        }
      ],
      "source": [
        "tokens = word_tokenize(raw)                                                                                      # assign tokenize raw to object called 'tokens'\n",
        "print(\"data type:\", type(tokens), \"Number of tokens:\", len(tokens))                                              # print datatype and number of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4935852",
      "metadata": {
        "id": "b4935852",
        "outputId": "0a3014cd-65b1-4bd6-f49c-54ee75a7b6bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Project',\n",
              " 'Gutenberg',\n",
              " 'eBook',\n",
              " 'of',\n",
              " 'Crime',\n",
              " 'and',\n",
              " 'Punishment',\n",
              " ',',\n",
              " 'by']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfcd0384",
      "metadata": {
        "id": "dfcd0384"
      },
      "outputs": [],
      "source": [
        "text = nltk.Text(tokens)                                                                                        # assign text tokens as an nltk Text to use nltk functions\n",
        "text.collocations()                                                                                             # pulls up common bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d947c4",
      "metadata": {
        "id": "f3d947c4"
      },
      "source": [
        "#### **4) Trimming unwanted text**\n",
        "\n",
        "If we examine the actual webpage of this project Gutenberg text, it's clear there's additional text that's not a part of the story. We can manually extract the part of the text we want using the `find` function and a regex pattern, and then overwrite our 'text' object to contain just that part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d520f5",
      "metadata": {
        "id": "a4d520f5",
        "outputId": "3c8e8f14-0d2f-44a4-a876-6f0cb517c625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 5574 : 1158052 ]\n"
          ]
        }
      ],
      "source": [
        "print(\"[\",raw.find(\"PART I\"), \":\", raw.rfind(\"END OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT\"), \"]\")   # locate the title and end of the book strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64fe383",
      "metadata": {
        "id": "b64fe383"
      },
      "outputs": [],
      "source": [
        "raw = raw[5574:1158052]                                                                                         # assign raw to just the text between the tile and end\n",
        "raw"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fdb0e7b",
      "metadata": {
        "id": "0fdb0e7b"
      },
      "source": [
        "### **Next, let's try out a  static HTML page**\n",
        " \n",
        "Non-Dynamic (or static) HTML pages like are a bit easier to deal with from a text-extraction standpoint because they don't have interactive components or java script. Less and less do we see static HTML in the wild, but it's still useful to understand the process and the associated functions. `BeautifulSoup` is a popular library used for web scraping, and we will implement it here.\n",
        "\n",
        "Here we will read in and then parse an example news article using `BeautifulSoup`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a75b1d",
      "metadata": {
        "id": "47a75b1d"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup                                                                                   # read in BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1) Read in raw text from url**\n",
        "#### **2) Decode text to readable format**"
      ],
      "metadata": {
        "id": "U0iBKkuDjscK"
      },
      "id": "U0iBKkuDjscK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f30358d",
      "metadata": {
        "id": "2f30358d"
      },
      "outputs": [],
      "source": [
        "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\" \n",
        "html = request.urlopen(url).read().decode('utf-8-sig')                                                         # decode url to utf-8\n",
        "print(html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab5c0ea",
      "metadata": {
        "id": "3ab5c0ea"
      },
      "source": [
        "**Oh man that's ugly. Let's implement the second part of step 2, which is necessary here: removal of HTML tags. This can be done using the html.parser from `BeautifulSoup`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b16d59",
      "metadata": {
        "id": "e9b16d59"
      },
      "outputs": [],
      "source": [
        "raw = BeautifulSoup(html, 'html.parser').get_text()                                                           # extract text, parse out html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Now let's complete steps 3) and 4), tokenizing and trimming:**\n",
        "\n",
        "This time we'll trim first, tokenize second -- really these steps can be done in any order."
      ],
      "metadata": {
        "id": "00HCUiorovpV"
      },
      "id": "00HCUiorovpV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e498506f",
      "metadata": {
        "id": "e498506f",
        "outputId": "a60cccb3-2712-4e21-ddf8-fc4fb02e6897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 943 : 2452 ]\n"
          ]
        }
      ],
      "source": [
        "print(\"[\",raw.find(\"The last natural blondes will die out within 200 years, scientists believe\"), \":\", raw.find(\"The frequency of blondes may drop but they won't disappear.\"), \"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd39abb",
      "metadata": {
        "id": "4fd39abb",
        "outputId": "f2ed9144-7d8a-40ea-c6c8-50ba35493bc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The last natural blondes will die out within 200 years, scientists believe. \\r\\n\\r\\nA study by experts in Germany suggests people with blonde hair are an endangered species and will become extinct by 2202.\\r\\n\\r\\nResearchers predict the last truly natural blonde will be born in Finland - the country with the highest proportion of blondes. \\r\\n\\n\\n\\n\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\tThe frequency of blondes may drop but they won\\'t disappear\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\t\\n\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\tProf Jonathan Rees, University of Edinburgh\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\t\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\t\\r\\nBut they say too few people now carry the gene for blondes to last beyond the next two centuries. \\r\\n\\r\\nThe problem is that blonde hair is caused by a recessive gene. \\r\\n\\r\\nIn order for a child to have blonde hair, it must have the gene on both sides of the family in the grandparents\\' generation. \\r\\nDyed rivals\\n\\r\\n\\r\\nThe researchers also believe that so-called bottle blondes may be to blame for the demise of their natural rivals. \\r\\n\\r\\nThey suggest that dyed-blondes are more attractive to men who choose them as partners over true blondes. \\r\\n\\n\\n\\n\\n\\nBottle-blondes like Ann Widdecombe may be to blame\\n\\n\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\nBut Jonathan Rees, professor of dermatology at the University of Edinburgh said it was unlikely blondes would die out completely. \\r\\n\\r\\n\"Genes don\\'t die out unless there is a disadvantage of having that gene or by chance. They don\\'t disappear,\" he told BBC News Online.\\r\\n\\r\\n\"The only reason blondes would disappear is if having the gene was a disadvantage and I do not think that is the case. \\r\\n\\r\\n\"The frequency of blondes may drop but they won\\'t disappear.\"'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_1 = raw[943:2512]\n",
        "raw_1                                                                                                          # trim text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146dc057",
      "metadata": {
        "id": "146dc057"
      },
      "outputs": [],
      "source": [
        "tokens = word_tokenize(raw_1)                                                                                   # tokenize the raw text to words\n",
        "text = nltk.Text(tokens)                                                                                        # set as our nltk text\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11dc2859",
      "metadata": {
        "id": "11dc2859",
        "outputId": "c37b2ca4-da9d-48ab-e35e-8e5596a87d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Displaying 5 of 5 matches:\n",
            "hey say too few people now carry the gene for blondes to last beyond the next \n",
            "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
            " have blonde hair , it must have the gene on both sides of the family in the g\n",
            "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
            "des would disappear is if having the gene was a disadvantage and I do not thin\n"
          ]
        }
      ],
      "source": [
        "text.concordance('gene')                                                                                        # search for context that the word 'gene' appears in"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7ebae0",
      "metadata": {
        "id": "4c7ebae0"
      },
      "source": [
        "### **Finally, we will scrape some text from Wikipedia, export it, and then read it in as a local file**\n",
        "\n",
        "We can actually access the wikipedia API through a handy-dandy `wikipedia` library in Python! Let's find some wikipedia pages about pengiuns that we could extract some text from..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a4024f",
      "metadata": {
        "id": "78a4024f"
      },
      "outputs": [],
      "source": [
        "#!pip install wikipedia\n",
        "import wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc55a288",
      "metadata": {
        "id": "bc55a288",
        "outputId": "ca1b35e3-a9e1-4570-b5eb-13f8cfea1030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Penguin', 'Pittsburgh Penguins', 'Adélie penguin', 'African penguin', 'Penguins of Madagascar', 'Little penguin', 'Penguin Books', 'The Penguins', 'Emperor penguin', 'King penguin']\n"
          ]
        }
      ],
      "source": [
        "print(wikipedia.search(\"Penguins\"))                                                         # search for pages about penguins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef3743af",
      "metadata": {
        "id": "ef3743af",
        "outputId": "4926b268-df29-499b-920b-a9ee8b4b1c9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "penguin books\n"
          ]
        }
      ],
      "source": [
        "print(wikipedia.suggest('Pengui Books'))                                                    # can make typos and it will make a suggestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6524e86",
      "metadata": {
        "id": "a6524e86",
        "outputId": "1f4202f8-74e4-499b-9972-058271fbf2d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<WikipediaPage 'Penguin Books'>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "wiki = wikipedia.page('penguin books')                                                      # we choose the penguin books wikipage and assign it to an object called 'wiki'\n",
        "wiki"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "350cc275",
      "metadata": {
        "id": "350cc275",
        "outputId": "18bba9eb-621f-4bb8-8342-1437406f00c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1450 : 5204 ]\n"
          ]
        }
      ],
      "source": [
        "print(\"[\",wiki.content.find(\"Origin\"), \":\", wiki.content.rfind(\"War years\"), \"]\")           # find text we want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef4282ba",
      "metadata": {
        "id": "ef4282ba",
        "outputId": "704aa908-df29-4f11-f9df-c1b59762f854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Origins ==\n",
            "\n",
            "The first Penguin paperbacks were published in 1935, but at first only as an imprint of The Bodley Head (of Vigo Street, London) with the books originally distributed from the crypt of Holy Trinity Church Marylebone.\n",
            "Anecdotally, Lane recounted how it was his experience with the poor quality of reading material on offer at Exeter train station that inspired him to create cheap, well designed quality books for the mass market. However the question of how publishers could reach a larger public had been the subject of a conference at Rippon Hall, Oxford in 1934 which Lane had attended. Though the publication of literature in paperback was then associated mainly with poor quality lurid fiction, the Penguin brand owed something to the short-lived Albatross imprint of British and American reprints that briefly traded in 1932.Inexpensive paperbacks did not initially appear viable to Bodley Head, since the deliberately low price of 6d. made profitability seem unlikely. This helped Lane purchase publication rights for some works more cheaply than he otherwise might have, since publishers were convinced of the business's short-term prospects. In the face of resistance from the traditional book trade, it was the purchase of 63,000 books by Woolworths Group that paid for the project outright, confirmed its worth, and allowed Lane to establish Penguin as a separate business in 1936. By March 1936, ten months after the company's launch on 30 July 1935, one million Penguin books had been printed.\n",
            "Only paperback editions were published until the King Penguin series debuted in 1939, and latterly the Pelican History of Art was undertaken; these works, considered unsuitable as paperbacks because of their lengths and copious illustrations on art paper, were cloth-bound.\n",
            "Penguin Books Inc was incorporated in 1939 to satisfy US copyright law; and, despite being a late entrant into an already well established paperback market, enjoyed further success under vice president Kurt Enoch with such titles as What Plane Is That and The New Soldier Handbook.\n",
            "The company's expansion saw the hiring of Eunice Frost—first as a secretary, then as editor, and ultimately as a director, who was to have a pivotal influence in shaping the company. In 1945 she was entrusted with the reconstruction of Penguin Inc after the departure of its first managing director, Ian Ballantine.From the outset, design was essential to Penguin's success. Avoiding the illustrated gaudiness of other paperback publishers, Penguin opted for the simple appearance of three horizontal bands, the upper and lower of which were colour-coded according to the series to which the title belonged; this is sometimes referred to as the horizontal grid. In the central white panel, the author and title were printed in Gill Sans, and in the upper band was a cartouche with the legend \"Penguin Books\". The initial design was created by then 21-year-old office junior Edward Young, who also drew the first version of the Penguin logo. Series such as Penguin Specials and The Penguin Shakespeare had individual designs (by 1937, only S1 and B1-B18 had been published).\n",
            "The colour schemes included: orange and white for general fiction, green and white for crime fiction, cerise and white for travel and adventure, dark blue and white for biographies, yellow and white for miscellaneous, red and white for drama; and the rarer purple and white for essays and belles lettres and grey and white for world affairs. Lane actively resisted the introduction of cover images for several years. Some recent publications of literature from that time have duplicated the original look.\n",
            "In 1937, Penguin's headquarters were established at Harmondsworth, close to Heathrow Airport.\n",
            "\n",
            "\n",
            "== \n"
          ]
        }
      ],
      "source": [
        "text = wiki.content[1450:5204]                                                              # trim text\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92de5ba3",
      "metadata": {
        "id": "92de5ba3"
      },
      "source": [
        "#### **Now let's export our wiki page text as a Text File:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18207823",
      "metadata": {
        "id": "18207823"
      },
      "outputs": [],
      "source": [
        "text_file = open('C:/users/carly/Documents/Text Mining Course/Penguins.txt', 'w')\n",
        "text_file.write(text)\n",
        "text_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be8330a",
      "metadata": {
        "id": "0be8330a"
      },
      "source": [
        "#### **And then read it back in again as a local file...**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288aa5a5",
      "metadata": {
        "id": "288aa5a5",
        "outputId": "4e7ab1c7-44e8-4401-ca8f-f6a73c1d2ba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Origins ==\n",
            "\n",
            "The first Penguin paperbacks were published in 1935, but at first only as an imprint of The Bodley Head (of Vigo Street, London) with the books originally distributed from the crypt of Holy Trinity Church Marylebone.\n",
            "Anecdotally, Lane recounted how it was his experience with the poor quality of reading material on offer at Exeter train station that inspired him to create cheap, well designed quality books for the mass market. However the question of how publishers could reach a larger public had been the subject of a conference at Rippon Hall, Oxford in 1934 which Lane had attended. Though the publication of literature in paperback was then associated mainly with poor quality lurid fiction, the Penguin brand owed something to the short-lived Albatross imprint of British and American reprints that briefly traded in 1932.Inexpensive paperbacks did not initially appear viable to Bodley Head, since the deliberately low price of 6d. made profitability seem unlikely. This helped Lane purchase publication rights for some works more cheaply than he otherwise might have, since publishers were convinced of the business's short-term prospects. In the face of resistance from the traditional book trade, it was the purchase of 63,000 books by Woolworths Group that paid for the project outright, confirmed its worth, and allowed Lane to establish Penguin as a separate business in 1936. By March 1936, ten months after the company's launch on 30 July 1935, one million Penguin books had been printed.\n",
            "Only paperback editions were published until the King Penguin series debuted in 1939, and latterly the Pelican History of Art was undertaken; these works, considered unsuitable as paperbacks because of their lengths and copious illustrations on art paper, were cloth-bound.\n",
            "Penguin Books Inc was incorporated in 1939 to satisfy US copyright law; and, despite being a late entrant into an already well established paperback market, enjoyed further success under vice president Kurt Enoch with such titles as What Plane Is That and The New Soldier Handbook.\n",
            "The company's expansion saw the hiring of Eunice Frost—first as a secretary, then as editor, and ultimately as a director, who was to have a pivotal influence in shaping the company. In 1945 she was entrusted with the reconstruction of Penguin Inc after the departure of its first managing director, Ian Ballantine.From the outset, design was essential to Penguin's success. Avoiding the illustrated gaudiness of other paperback publishers, Penguin opted for the simple appearance of three horizontal bands, the upper and lower of which were colour-coded according to the series to which the title belonged; this is sometimes referred to as the horizontal grid. In the central white panel, the author and title were printed in Gill Sans, and in the upper band was a cartouche with the legend \"Penguin Books\". The initial design was created by then 21-year-old office junior Edward Young, who also drew the first version of the Penguin logo. Series such as Penguin Specials and The Penguin Shakespeare had individual designs (by 1937, only S1 and B1-B18 had been published).\n",
            "The colour schemes included: orange and white for general fiction, green and white for crime fiction, cerise and white for travel and adventure, dark blue and white for biographies, yellow and white for miscellaneous, red and white for drama; and the rarer purple and white for essays and belles lettres and grey and white for world affairs. Lane actively resisted the introduction of cover images for several years. Some recent publications of literature from that time have duplicated the original look.\n",
            "In 1937, Penguin's headquarters were established at Harmondsworth, close to Heathrow Airport.\n",
            "\n",
            "\n",
            "==\n"
          ]
        }
      ],
      "source": [
        "f = open('C:/users/carly/Documents/Text Mining Course/Penguins.txt', 'r')\n",
        "for line in f:\n",
        "    print(line.strip())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
