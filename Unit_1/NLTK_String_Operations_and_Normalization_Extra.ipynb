{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "69806e62",
      "metadata": {
        "id": "69806e62"
      },
      "source": [
        "## Strings: Text Proprocessing & Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11af41fd",
      "metadata": {
        "id": "11af41fd"
      },
      "source": [
        "### Basic String Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c80b22a",
      "metadata": {
        "id": "1c80b22a"
      },
      "outputs": [],
      "source": [
        "string1 = 'The FIFA World Cup'\n",
        "string2 = \"in Ballon d'Or\"                                                                          # double quotes if inner string contains single apostrophe\n",
        "string3 = \"was Jessie's favorite match.\"                                                            # same here\n",
        "\n",
        "print(string1 + \" \" + string2 + \" \" + string3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95733881",
      "metadata": {
        "id": "95733881"
      },
      "outputs": [],
      "source": [
        "'mm' * 3                                                                                            # you can also multiply strings but division or subtraction doesn't work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578ae7e0",
      "metadata": {
        "id": "578ae7e0"
      },
      "outputs": [],
      "source": [
        "a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n",
        "b = [' ' * 2 * (7 - i) + 'oooo' * i for i in a]\n",
        "for line in b:\n",
        "    print(line)                                                                                     #this is fun, let's try to figure out what this code will do"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "310fa95d",
      "metadata": {
        "id": "310fa95d"
      },
      "source": [
        "## Accessing substrings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3875deba",
      "metadata": {
        "id": "3875deba"
      },
      "outputs": [],
      "source": [
        "string1[4:18]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8237abf2",
      "metadata": {
        "id": "8237abf2"
      },
      "outputs": [],
      "source": [
        "phrase = 'download webpage, strip HTML if necessary, trim to desired length'\n",
        "\n",
        "if 'down' in phrase:\n",
        "    print('found \"down\"', \"| position = \", phrase.find('down'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf2a21d",
      "metadata": {
        "id": "2bf2a21d"
      },
      "source": [
        "## Combining regex with string operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98112c0e",
      "metadata": {
        "id": "98112c0e"
      },
      "source": [
        "### Extracting word pieces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2aa39fa",
      "metadata": {
        "id": "d2aa39fa"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b23b83",
      "metadata": {
        "id": "11b23b83"
      },
      "outputs": [],
      "source": [
        "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
        "fd = nltk.FreqDist(va for word in wsj\n",
        "                  for va in re.findall(r'[aeiou]{3,}', word))                                               # find most common occurances of words with three vowel combos\n",
        "fd.most_common(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6a0b02",
      "metadata": {
        "id": "9f6a0b02"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import words\n",
        "wiki_words = words.words()\n",
        "wiki_words = re.findall(r'\\b\\w{5}\\b', str(wiki_words)) #find all 5 letter words\n",
        "cvs = [cv for w in wiki_words for cv in re.findall(r'^[qwrtpsdfghjklzxcvbnm][aeiou]', w)]                   # findall starting consonant-vowel combos\n",
        "cfd = nltk.ConditionalFreqDist(cvs)\n",
        "cfd.tabulate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60669c11",
      "metadata": {
        "id": "60669c11"
      },
      "source": [
        "### Regex for searching across multiple words in a text -- seeing words in context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef54f65",
      "metadata": {
        "id": "9ef54f65",
        "outputId": "44e4fc19-9f01-4a75-c3c4-de5b3bd9eded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "you rule bro; telling you bro; u twizted bro\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import nps_chat\n",
        "\n",
        "chat = nltk.Text(nps_chat.words())\n",
        "chat.findall(r\"<.*><.*><bro>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1fe05f2",
      "metadata": {
        "id": "d1fe05f2",
        "outputId": "ad572a0c-8c5b-47f5-f4c0-796a9ee720bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speed and other activities; water and other liquids; tomb and other\n",
            "landmarks; Statues and other monuments; pearls and other jewels;\n",
            "charts and other items; roads and other features; figures and other\n",
            "objects; military and other areas; demands and other factors;\n",
            "abstracts and other compilations; iron and other metals\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
        "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792f96f1",
      "metadata": {
        "id": "792f96f1"
      },
      "source": [
        "## Normalizing Text - pretrained and deterministic approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0283230",
      "metadata": {
        "id": "c0283230"
      },
      "source": [
        "### Using regex to stem words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ffc43cb",
      "metadata": {
        "id": "0ffc43cb",
        "outputId": "00e67284-be68-468d-a2c4-12c42dcae99c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'forgiv'"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def stem(word):\n",
        "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ive', 'es', 's', 'ment', 'en', 'ness']:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "stem('forgiving')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75be3044",
      "metadata": {
        "id": "75be3044",
        "outputId": "036b6673-5efd-4e9d-de17-abb2d21a7b8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('processe', 's')]"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.findall(r'(^.*)(ing|ly|ed|ious|ies|ive|es|s|ment|en|ness)$', 'processes' ) #greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77d0c33",
      "metadata": {
        "id": "a77d0c33",
        "outputId": "59bc6363-ae97-4106-baf7-7e0da3b90f8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('process', 'es')]"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.findall(r'(^.*?)(ing|ly|ed|ious|ies|ive|es|s|ment|en|ness)$', 'processes' ) #non-greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a01bad91",
      "metadata": {
        "id": "a01bad91",
        "outputId": "cf9ab739-1295-47eb-fa8f-cc8c3403f820"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A',\n",
              " 'a',\n",
              " 'young',\n",
              " 'industry',\n",
              " 'that',\n",
              " 'champion',\n",
              " 'innovation',\n",
              " 'and',\n",
              " 'rat',\n",
              " 'it',\n",
              " 'practitioner',\n",
              " 'bas',\n",
              " 'on',\n",
              " 'their',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'apprehend',\n",
              " '(',\n",
              " 'sorry',\n",
              " ',',\n",
              " 'grok',\n",
              " ')',\n",
              " 'the',\n",
              " 'continual',\n",
              " 'emergence']"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def stem(word):\n",
        "    regex = r'(^.*?)(ing|ly|ed|ious|ies|ive|es|s|ment|en|ness)?$'\n",
        "    stem, suffix = re.findall(regex, word)[0]\n",
        "    return stem\n",
        "\n",
        "raw = \"\"\"As a young industry that champions innovation and rates its practitioners based\n",
        "on their ability to apprehend (sorry, grok) the continual emergence of new technologies,\n",
        "frameworks, protocols and data models, we are not particularly familiar with tradition. \n",
        "However, the practice of arranging type for optimal pleasure and comfort is a centuries\n",
        "-old discipline. As long ago as 1927, the noted typographer Jan Tschichold spoke of the\n",
        "typesetting 'methods and rules upon which it is impossible to improve' — a set of rules\n",
        "it would be foolish to ignore.\"\"\"\n",
        "\n",
        "tokens = word_tokenize(raw)\n",
        "[stem(t) for t in tokens][0:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb69520",
      "metadata": {
        "id": "dbb69520"
      },
      "source": [
        "### Porter Stemmer & WordNet Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e441a34",
      "metadata": {
        "id": "7e441a34"
      },
      "outputs": [],
      "source": [
        "raw = \"\"\"As a young industry that champions innovation and rates its practitioners based\n",
        "on their ability to apprehend (sorry, grok) the continual emergence of new technologies,\n",
        "frameworks, protocols and data models, we are not particularly familiar with tradition. \n",
        "However, the practice of arranging type for optimal pleasure and comfort is a centuries\n",
        "-old discipline. As long ago as 1927, the noted typographer Jan Tschichold spoke of the\n",
        "typesetting 'methods and rules upon which it is impossible to improve' — a set of rules\n",
        "it would be foolish to ignore.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a90a32",
      "metadata": {
        "id": "f4a90a32",
        "outputId": "6166a5aa-5a3c-4054-fb6f-1897e57f7b6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['as',\n",
              " 'a',\n",
              " 'young',\n",
              " 'industry',\n",
              " 'that',\n",
              " 'champions',\n",
              " 'innovation',\n",
              " 'and',\n",
              " 'rates',\n",
              " 'its',\n",
              " 'practitioners',\n",
              " 'based',\n",
              " 'on',\n",
              " 'their',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'apprehend',\n",
              " '(',\n",
              " 'sorry',\n",
              " ',',\n",
              " 'grok',\n",
              " ')',\n",
              " 'the',\n",
              " 'continual',\n",
              " 'emergence',\n",
              " 'of',\n",
              " 'new',\n",
              " 'technologies',\n",
              " ',',\n",
              " 'frameworks']"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = word_tokenize(raw) #tokenize the text\n",
        "tokens = [w.lower() for w in tokens] #set all words to lowercase\n",
        "tokens[0:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d626b36",
      "metadata": {
        "id": "5d626b36",
        "outputId": "5c25b042-7bb1-401d-bd7a-696df381187d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['as',\n",
              " 'a',\n",
              " 'young',\n",
              " 'industri',\n",
              " 'that',\n",
              " 'champion',\n",
              " 'innov',\n",
              " 'and',\n",
              " 'rate',\n",
              " 'it',\n",
              " 'practition',\n",
              " 'base',\n",
              " 'on',\n",
              " 'their',\n",
              " 'abil',\n",
              " 'to',\n",
              " 'apprehend',\n",
              " '(',\n",
              " 'sorri',\n",
              " ',',\n",
              " 'grok',\n",
              " ')',\n",
              " 'the',\n",
              " 'continu',\n",
              " 'emerg',\n",
              " 'of',\n",
              " 'new',\n",
              " 'technolog',\n",
              " ',',\n",
              " 'framework']"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "stemmed = [porter.stem(t) for t in tokens]\n",
        "stemmed[0:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c76d34",
      "metadata": {
        "id": "21c76d34",
        "outputId": "655b4916-2a49-4718-f1d0-adaa6a50e58f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a',\n",
              " 'a',\n",
              " 'young',\n",
              " 'industry',\n",
              " 'that',\n",
              " 'champion',\n",
              " 'innovation',\n",
              " 'and',\n",
              " 'rate',\n",
              " 'it',\n",
              " 'practitioner',\n",
              " 'based',\n",
              " 'on',\n",
              " 'their',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'apprehend',\n",
              " '(',\n",
              " 'sorry',\n",
              " ',',\n",
              " 'grok',\n",
              " ')',\n",
              " 'the',\n",
              " 'continual',\n",
              " 'emergence',\n",
              " 'of',\n",
              " 'new',\n",
              " 'technology',\n",
              " ',',\n",
              " 'framework']"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "lemma = [wnl.lemmatize(t) for t in tokens]\n",
        "lemma[0:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db3211bf",
      "metadata": {
        "id": "db3211bf"
      },
      "source": [
        "### Regex for Deterministic Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "728fd560",
      "metadata": {
        "id": "728fd560",
        "outputId": "72fcd3e6-e556-4c68-ad7e-a7668b751c73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['As', 'a', 'young', 'industry', 'that', 'champions', 'innovation', 'and', 'rates', 'its', 'practitioners', 'based\\non', 'their', 'ability', 'to', 'apprehend', '(sorry,', 'grok)', 'the', 'continual', 'emergence', 'of', 'new', 'technologies,\\nframeworks,', 'protocols', 'and', 'data', 'models,', 'we', 'are', 'not', 'particularly', 'familiar', 'with', 'tradition.', '\\nHowever,', 'the', 'practice', 'of', 'arranging', 'type', 'for', 'optimal', 'pleasure', 'and', 'comfort', 'is', 'a', 'centuries\\n-old', 'discipline.', 'As', 'long', 'ago', 'as', '1927,', 'the', 'noted', 'typographer', 'Jan', 'Tschichold', 'spoke', 'of', 'the\\ntypesetting', \"'methods\", 'and', 'rules', 'upon', 'which', 'it', 'is', 'impossible', 'to', \"improve'\", '—', 'a', 'set', 'of', 'rules\\nit', 'would', 'be', 'foolish', 'to', 'ignore.']\n"
          ]
        }
      ],
      "source": [
        "print(re.split(r' ', raw)) #splitting based on spaces in the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb16ada6",
      "metadata": {
        "id": "eb16ada6",
        "outputId": "0f9e70b7-4fc4-4180-92c1-9576cde9bfe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['As', 'a', 'young', 'industry', 'that', 'champions', 'innovation', 'and', 'rates', 'its', 'practitioners', 'based', 'on', 'their', 'ability', 'to', 'apprehend', 'sorry', 'grok', 'the', 'continual', 'emergence', 'of', 'new', 'technologies', 'frameworks', 'protocols', 'and', 'data', 'models', 'we', 'are', 'not', 'particularly', 'familiar', 'with', 'tradition', 'However', 'the', 'practice', 'of', 'arranging', 'type', 'for', 'optimal', 'pleasure', 'and', 'comfort', 'is', 'a', 'centuries', 'old', 'discipline', 'As', 'long', 'ago', 'as', '1927', 'the', 'noted', 'typographer', 'Jan', 'Tschichold', 'spoke', 'of', 'the', 'typesetting', 'methods', 'and', 'rules', 'upon', 'which', 'it', 'is', 'impossible', 'to', 'improve', 'a', 'set', 'of', 'rules', 'it', 'would', 'be', 'foolish', 'to', 'ignore', '']\n"
          ]
        }
      ],
      "source": [
        "print(re.split(r'\\W+', raw)) #split on anything other than a word character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f546fdaf",
      "metadata": {
        "id": "f546fdaf",
        "outputId": "8e6b0fe3-50ce-485e-f593-a926fe346332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['As', 'a', 'young', 'industry', 'that', 'champions', 'innovation', 'and', 'rates', 'its', 'practitioners', 'based', 'on', 'their', 'ability', 'to', 'apprehend', '(sorry', ',', 'grok', ')', 'the', 'continual', 'emergence', 'of', 'new', 'technologies', ',', 'frameworks', ',', 'protocols', 'and', 'data', 'models', ',', 'we', 'are', 'not', 'particularly', 'familiar', 'with', 'tradition', '.', 'However', ',', 'the', 'practice', 'of', 'arranging', 'type', 'for', 'optimal', 'pleasure', 'and', 'comfort', 'is', 'a', 'centuries', '-old', 'discipline', '.', 'As', 'long', 'ago', 'as', '1927', ',', 'the', 'noted', 'typographer', 'Jan', 'Tschichold', 'spoke', 'of', 'the', 'typesetting', \"'methods\", 'and', 'rules', 'upon', 'which', 'it', 'is', 'impossible', 'to', 'improve', \"'\", '—', 'a', 'set', 'of', 'rules', 'it', 'would', 'be', 'foolish', 'to', 'ignore', '.']\n"
          ]
        }
      ],
      "source": [
        "print(re.findall(r'\\w+|\\S\\w*', raw))\n",
        "#first find any sequence of words characters or non-whitespace character followed by word characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b3232d",
      "metadata": {
        "scrolled": true,
        "id": "11b3232d",
        "outputId": "85ab0c96-226e-49c4-8661-7084887d86a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['As', 'a', 'young', 'industry', 'that', 'champions', 'innovation', 'and', 'rates', 'its', 'practitioners', 'based', 'on', 'their', 'ability', 'to', 'apprehend', '(', 'sorry', ',', 'grok', ')', 'the', 'continual', 'emergence', 'of', 'new', 'technologies', ',', 'frameworks', ',', 'protocols', 'and', 'data', 'models', ',', 'we', 'are', 'not', 'particularly', 'familiar', 'with', 'tradition', '.', 'However', ',', 'the', 'practice', 'of', 'arranging', 'type', 'for', 'optimal', 'pleasure', 'and', 'comfort', 'is', 'a', 'centuries', '-', 'old', 'discipline', '.', 'As', 'long', 'ago', 'as', '1927', ',', 'the', 'noted', 'typographer', 'Jan', 'Tschichold', 'spoke', 'of', 'the', 'typesetting', \"'\", 'methods', 'and', 'rules', 'upon', 'which', 'it', 'is', 'impossible', 'to', 'improve', \"'\", '—', 'a', 'set', 'of', 'rules', 'it', 'would', 'be', 'foolish', 'to', 'ignore', '.']\n"
          ]
        }
      ],
      "source": [
        "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))\n",
        "#/\\w+([-']\\w+)/ permit word-internal hyphens separate\n",
        "#|'| keep quote characters separate from the text they enclose\n",
        "#[-.(]+ double hyphen, ellipses, and open parantheses tokenized separately"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec0ac07b",
      "metadata": {
        "id": "ec0ac07b"
      },
      "source": [
        "### NLTK Regex Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db8466c",
      "metadata": {
        "id": "8db8466c",
        "outputId": "1c179a29-3be3-4dac-d04c-010dd6b951ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A',\n",
              " 'plane',\n",
              " 'ticket',\n",
              " 'to',\n",
              " 'Washington',\n",
              " 'D.C.',\n",
              " 'from',\n",
              " 'S.L.C.',\n",
              " 'costs',\n",
              " '$496.78',\n",
              " 'round-trip',\n",
              " '...']"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'A plane ticket to Washington D.C. from S.L.C. costs $496.78 round-trip...'\n",
        "\n",
        "pattern = r'''(?x)       # set flag to allow verbose (multiple) regex patterns\n",
        "        (?:[A-Z]\\.)+     # abbreviations, e.g., D.C., S.L.C\n",
        "    | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
        "    | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages\n",
        "    | \\.\\.\\.             # ellipsis\n",
        "    | [][.,:\"'?():-_`]   # these are separate tokens; includes ], [\n",
        "    '''\n",
        "nltk.regexp_tokenize(text, pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "056ddb33",
      "metadata": {
        "id": "056ddb33"
      },
      "source": [
        "## Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abe39c6",
      "metadata": {
        "id": "5abe39c6"
      },
      "outputs": [],
      "source": [
        "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0fcc004",
      "metadata": {
        "id": "b0fcc004",
        "outputId": "dc3ac99f-d1a7-4acd-9d85-e32ac5f31114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Weak if we were and foolish, not thus we failed, not thus;\\nWhen that black Baal blocked the heavens he had no hymns from us\\nChildren we were--our forts of sand were even as weak as eve,\\nHigh as they went we piled them up to break that bitter sea.',\n",
              " 'Fools as we were in motley, all jangling and absurd,\\nWhen all church bells were silent our cap and beds were heard.',\n",
              " 'Not all unhelped we held the fort, our tiny flags unfurled;\\nSome giants laboured in that cloud to lift it from the world.',\n",
              " 'I find again the book we found, I feel the hour that flings\\nFar out of fish-shaped Paumanok some cry of cleaner things;\\nAnd the Green Carnation withered, as in forest fires that pass,\\nRoared in the wind of all the world ten million leaves of grass;\\nOr sane and sweet and sudden as a bird sings in the rain--\\nTruth out of Tusitala spoke and pleasure out of pain.',\n",
              " 'Yea, cool and clear and sudden as a bird sings in the grey,\\nDunedin to Samoa spoke, and darkness unto day.']"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents = nltk.sent_tokenize(text)\n",
        "sents[5:10]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}