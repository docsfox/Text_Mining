{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dcK2l79Gb25y"},"outputs":[],"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.tokenize.toktok import ToktokTokenizer\n","tokenizer = ToktokTokenizer()\n","stopword_list = nltk.corpus.stopwords.words('english')\n","from nltk.stem import WordNetLemmatizer\n","wnl = WordNetLemmatizer()\n","\n","from pprint import pprint\n","import numpy as np\n","import re\n","from bs4 import BeautifulSoup\n","\n","import spacy\n","nlp = spacy.load('en_core_web_sm')                                                                                            # dependencies\n","\n","import unicodedata\n","\n","!pip install contractions\n","import contractions\n","\n","def strip_html_tags(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    [s.extract() for s in soup(['iframe', 'script'])]                                                                         # html parsing\n","    stripped_text = soup.get_text()\n","    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n","    return stripped_text\n","\n","def tokenize_text(text):                                                                                                      # text tokenization\n","    sentences = nltk.sent_tokenize(text)\n","    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] \n","    return word_tokens\n","\n","def remove_accented_chars(text):\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')                            # accent removal\n","    return text\n","\n","def expand_contractions(text):                                                                                                # expand contractions\n","    expanded_words = []\n","    for word in text.split():\n","        expanded_words.append(contractions.fix(word))\n","        expanded_text = ' '.join(expanded_words)\n","    return expanded_text\n","\n","def remove_special_characters(text, remove_digits=False):                                                                    # special character removal\n","    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n","    text = re.sub(pattern, '', text)\n","    return text\n","\n","def simple_stemmer(text):                                                                                                   # stemmer\n","    ps = nltk.porter.PorterStemmer()\n","    text = ' '.join([ps.stem(word) for word in text.split()])\n","    return text\n","\n","def lemmatize_text(text):\n","    text = nlp(text)                                                                                                        # lemmatizer\n","    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n","    return text\n","\n","def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):                                                   # stopword removal\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    if is_lower_case:\n","        filtered_tokens = [token for token in tokens if token not in stopwords]\n","    else:\n","        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n","    filtered_text = ' '.join(filtered_tokens)    \n","    return filtered_text\n","\n","def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,                                                # define normalize corpus function\n","                     accented_char_removal=True, text_lower_case=True, \n","                     text_lemmatization=True, special_char_removal=True, \n","                     stopword_removal=True, remove_digits=True):\n","    \n","    normalized_corpus = []\n","    # normalize each document in the corpus\n","    for doc in corpus:\n","        # strip HTML\n","        if html_stripping:\n","            doc = strip_html_tags(doc)\n","        # remove accented characters\n","        if accented_char_removal:\n","            doc = remove_accented_chars(doc)\n","        # expand contractions    \n","        if contraction_expansion:\n","            doc = expand_contractions(doc)\n","        # lowercase the text    \n","        if text_lower_case:\n","            doc = doc.lower()\n","        # remove extra newlines\n","        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n","        # lemmatize text\n","        if text_lemmatization:\n","            doc = lemmatize_text(doc)\n","        # remove special characters and\\or digits    \n","        if special_char_removal:\n","            # insert spaces between special characters to isolate them    \n","            special_char_pattern = re.compile(r'([{.(-)!}])')\n","            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n","            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n","        # remove extra whitespace\n","        doc = re.sub(' +', ' ', doc)\n","        # remove stopwords\n","        if stopword_removal:\n","            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n","            \n","        normalized_corpus.append(doc)\n","        \n","    return normalized_corpus"]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}